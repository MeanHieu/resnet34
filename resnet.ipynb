{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'vi_VN.UTF-8'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import threading\n",
    "import os\n",
    "import cv2\n",
    "import face_recognition\n",
    "import numpy as np\n",
    "import tkinter as tk\n",
    "from tkinter import scrolledtext\n",
    "from PIL import Image, ImageTk\n",
    "from datetime import datetime\n",
    "import locale\n",
    "import keyboard\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Đặt lại môi trường locale để hỗ trợ Unicode\n",
    "locale.setlocale(locale.LC_ALL, 'vi_VN.UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'student_images'  # Thay đổi thành đường dẫn thư mục chứa hình ảnh "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "### SETTINGS\n",
    "##########################\n",
    "\n",
    "# Hyperparameters\n",
    "RANDOM_SEED = 1\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "# Architecture\n",
    "NUM_FEATURES = 128*128\n",
    "NUM_CLASSES = len(path)\n",
    "BATCH_SIZE = 256*torch.cuda.device_count()\n",
    "DEVICE = 'cuda:0' # default GPU device\n",
    "GRAYSCALE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 37 images and corresponding names and info.\n"
     ]
    }
   ],
   "source": [
    "images = []\n",
    "classNames = []\n",
    "classInfo = {}  # Từ điển để lưu thông tin từ tệp .txt\n",
    "classNames1 = []\n",
    "# Duyệt qua các thư mục và tệp tin trong `path`\n",
    "for root, dirs, files in os.walk(path):\n",
    "    for cl in dirs:  # Duyệt qua các thư mục con\n",
    "        class_path = os.path.join(root, cl)\n",
    "        for img_name in os.listdir(class_path):\n",
    "            img_path = os.path.join(class_path, img_name)\n",
    "            if img_name.endswith(('png', 'jpg', 'jpeg')):\n",
    "                curImg = cv2.imread(img_path)\n",
    "                if curImg is not None:\n",
    "                    images.append(curImg)\n",
    "                    txt_path = os.path.join(class_path, cl + '.txt')\n",
    "                    if os.path.exists(txt_path):\n",
    "                        with open(txt_path, 'r', encoding='utf-8') as file:\n",
    "                            content = file.read().strip().splitlines()\n",
    "                            name = content[0].strip()  # Lấy tên từ dòng đầu tiên\n",
    "                            info = '\\n'.join(content[1:]).strip()  # Lấy thông tin từ các dòng còn lại\n",
    "                            classNames.append(cl)\n",
    "                            classNames1.append(name)\n",
    "                            classInfo[name] = info\n",
    "                    else:\n",
    "                        print(f\"Error: Could not find the text file {txt_path}\")\n",
    "                else:\n",
    "                    print(f\"Error: Could not load image {img_path}\")\n",
    "\n",
    "# Hiển thị số lượng hình ảnh đã được nạp\n",
    "print(f\"Loaded {len(images)} images and corresponding names and info.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training images: 29\n",
      "Number of test images: 8\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import cv2\n",
    "\n",
    "# Đảm bảo rằng ảnh có cùng kích thước\n",
    "def resize_image(image, size=(224, 224)):\n",
    "    \"\"\"Resize image to the given size.\"\"\"\n",
    "    return cv2.resize(image, size)\n",
    "\n",
    "# Chuyển đổi dữ liệu thành numpy array\n",
    "resized_images = [resize_image(img) for img in images]\n",
    "images_array = np.array(resized_images)\n",
    "classNames_array = np.array(classNames)\n",
    "\n",
    "# Chia dữ liệu thành tập huấn luyện và kiểm tra\n",
    "X_train, X_test, y_train, y_test = train_test_split(images_array, classNames_array, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit LabelEncoder với toàn bộ dữ liệu\n",
    "all_labels = np.concatenate([y_train, y_test])\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(all_labels)\n",
    "\n",
    "# Chuyển đổi nhãn\n",
    "y_train_encoded = label_encoder.transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# One-hot encoding\n",
    "y_train_cat = to_categorical(y_train_encoded)\n",
    "y_test_cat = to_categorical(y_test_encoded)\n",
    "\n",
    "# Chuyển đổi dữ liệu thành tensor nếu bạn đang dùng PyTorch\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Chuyển đổi ảnh thành tensor\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize nếu cần\n",
    "])\n",
    "\n",
    "X_train_tensor = torch.stack([transform(image) for image in X_train])\n",
    "X_test_tensor = torch.stack([transform(image) for image in X_test])\n",
    "\n",
    "# Chuyển dữ liệu thành dạng Tensor\n",
    "y_train_tensor = torch.tensor(y_train_encoded, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test_encoded, dtype=torch.long)\n",
    "\n",
    "# Hiển thị số lượng hình ảnh đã được nạp\n",
    "print(f\"Number of training images: {len(X_train)}\")\n",
    "print(f\"Number of test images: {len(X_test)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tạo DataLoader cho tập huấn luyện và tập kiểm tra\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Định nghĩa mô hình ResNet-34\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, num_classes, grayscale):\n",
    "        self.inplanes = 64\n",
    "        if grayscale:\n",
    "            in_dim = 1\n",
    "        else:\n",
    "            in_dim = 3\n",
    "        super(ResNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_dim, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "        self.avgpool = nn.AvgPool2d(7, stride=1, padding=2)\n",
    "        self.fc = nn.Linear(512 * 5 * 5, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, (2. / n)**.5)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        print(\"Shape before flattening:\", x.shape)  # In kích thước tensor ở đây\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        print(\"Shape after flattening:\", x.shape)  # In kích thước tensor sau khi flatten\n",
    "        logits = self.fc(x)\n",
    "        probas = F.softmax(logits, dim=1)\n",
    "        return logits, probas\n",
    "\n",
    "def resnet34(num_classes, grayscale):\n",
    "    \"\"\"Constructs a ResNet-34 model.\"\"\"\n",
    "    model = ResNet(block=BasicBlock, layers=[3, 4, 6, 3], num_classes=num_classes, grayscale=grayscale)\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 10\n"
     ]
    }
   ],
   "source": [
    "# Khởi tạo mô hình ResNet-34\n",
    "from torchvision import datasets, transforms\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    datasets.ImageFolder('student_images', transform=transform),\n",
    "    batch_size=1,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Xác định số lớp\n",
    "num_classes = len(data_loader.dataset.classes)\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "grayscale = False  # Thay đổi tùy theo dữ liệu của bạn\n",
    "model = resnet34(num_classes, grayscale)\n",
    "\n",
    "# Định nghĩa criterion và optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Kỹ thuật giảm tốc độ học\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5)\n",
    "\n",
    "# Callback: Lưu trọng số mô hình tốt nhất\n",
    "best_acc = 0.0\n",
    "model_save_path = 'best_model.pth'\n",
    "\n",
    "# Callback: Early Stopping\n",
    "patience = 5\n",
    "early_stop_counter = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before flattening: torch.Size([29, 512, 5, 5])\n",
      "Shape after flattening: torch.Size([29, 12800])\n",
      "Shape before flattening: torch.Size([8, 512, 5, 5])\n",
      "Shape after flattening: torch.Size([8, 12800])\n",
      "Epoch 1/50, Loss: 2.2615, Train Accuracy: 13.79%, Test Accuracy: 12.50%\n",
      "Shape before flattening: torch.Size([29, 512, 5, 5])\n",
      "Shape after flattening: torch.Size([29, 12800])\n",
      "Shape before flattening: torch.Size([8, 512, 5, 5])\n",
      "Shape after flattening: torch.Size([8, 12800])\n",
      "Epoch 2/50, Loss: 2.1481, Train Accuracy: 13.79%, Test Accuracy: 12.50%\n",
      "Shape before flattening: torch.Size([29, 512, 5, 5])\n",
      "Shape after flattening: torch.Size([29, 12800])\n",
      "Shape before flattening: torch.Size([8, 512, 5, 5])\n",
      "Shape after flattening: torch.Size([8, 12800])\n",
      "Epoch 3/50, Loss: 2.0164, Train Accuracy: 13.79%, Test Accuracy: 25.00%\n",
      "Shape before flattening: torch.Size([29, 512, 5, 5])\n",
      "Shape after flattening: torch.Size([29, 12800])\n",
      "Shape before flattening: torch.Size([8, 512, 5, 5])\n",
      "Shape after flattening: torch.Size([8, 12800])\n",
      "Epoch 4/50, Loss: 1.9060, Train Accuracy: 20.69%, Test Accuracy: 25.00%\n",
      "Shape before flattening: torch.Size([29, 512, 5, 5])\n",
      "Shape after flattening: torch.Size([29, 12800])\n",
      "Shape before flattening: torch.Size([8, 512, 5, 5])\n",
      "Shape after flattening: torch.Size([8, 12800])\n",
      "Epoch 5/50, Loss: 1.8541, Train Accuracy: 24.14%, Test Accuracy: 12.50%\n",
      "Shape before flattening: torch.Size([29, 512, 5, 5])\n",
      "Shape after flattening: torch.Size([29, 12800])\n",
      "Shape before flattening: torch.Size([8, 512, 5, 5])\n",
      "Shape after flattening: torch.Size([8, 12800])\n",
      "Epoch 6/50, Loss: 1.8209, Train Accuracy: 34.48%, Test Accuracy: 12.50%\n",
      "Shape before flattening: torch.Size([29, 512, 5, 5])\n",
      "Shape after flattening: torch.Size([29, 12800])\n",
      "Shape before flattening: torch.Size([8, 512, 5, 5])\n",
      "Shape after flattening: torch.Size([8, 12800])\n",
      "Epoch 7/50, Loss: 1.7940, Train Accuracy: 27.59%, Test Accuracy: 12.50%\n",
      "Shape before flattening: torch.Size([29, 512, 5, 5])\n",
      "Shape after flattening: torch.Size([29, 12800])\n",
      "Shape before flattening: torch.Size([8, 512, 5, 5])\n",
      "Shape after flattening: torch.Size([8, 12800])\n",
      "Epoch 8/50, Loss: 1.7726, Train Accuracy: 34.48%, Test Accuracy: 0.00%\n",
      "Shape before flattening: torch.Size([29, 512, 5, 5])\n",
      "Shape after flattening: torch.Size([29, 12800])\n",
      "Shape before flattening: torch.Size([8, 512, 5, 5])\n",
      "Shape after flattening: torch.Size([8, 12800])\n",
      "Epoch 9/50, Loss: 1.7558, Train Accuracy: 34.48%, Test Accuracy: 12.50%\n",
      "Shape before flattening: torch.Size([29, 512, 5, 5])\n",
      "Shape after flattening: torch.Size([29, 12800])\n",
      "Shape before flattening: torch.Size([8, 512, 5, 5])\n",
      "Shape after flattening: torch.Size([8, 12800])\n",
      "Epoch 10/50, Loss: 1.7407, Train Accuracy: 34.48%, Test Accuracy: 12.50%\n",
      "Shape before flattening: torch.Size([29, 512, 5, 5])\n",
      "Shape after flattening: torch.Size([29, 12800])\n",
      "Shape before flattening: torch.Size([8, 512, 5, 5])\n",
      "Shape after flattening: torch.Size([8, 12800])\n",
      "Epoch 11/50, Loss: 1.7241, Train Accuracy: 34.48%, Test Accuracy: 0.00%\n",
      "Shape before flattening: torch.Size([29, 512, 5, 5])\n",
      "Shape after flattening: torch.Size([29, 12800])\n",
      "Shape before flattening: torch.Size([8, 512, 5, 5])\n",
      "Shape after flattening: torch.Size([8, 12800])\n",
      "Epoch 12/50, Loss: 1.7041, Train Accuracy: 34.48%, Test Accuracy: 0.00%\n",
      "Shape before flattening: torch.Size([29, 512, 5, 5])\n",
      "Shape after flattening: torch.Size([29, 12800])\n",
      "Shape before flattening: torch.Size([8, 512, 5, 5])\n",
      "Shape after flattening: torch.Size([8, 12800])\n",
      "Epoch 13/50, Loss: 1.6822, Train Accuracy: 37.93%, Test Accuracy: 0.00%\n",
      "Shape before flattening: torch.Size([29, 512, 5, 5])\n",
      "Shape after flattening: torch.Size([29, 12800])\n",
      "Shape before flattening: torch.Size([8, 512, 5, 5])\n",
      "Shape after flattening: torch.Size([8, 12800])\n",
      "Epoch 14/50, Loss: 1.6613, Train Accuracy: 37.93%, Test Accuracy: 0.00%\n",
      "Shape before flattening: torch.Size([29, 512, 5, 5])\n",
      "Shape after flattening: torch.Size([29, 12800])\n",
      "Shape before flattening: torch.Size([8, 512, 5, 5])\n",
      "Shape after flattening: torch.Size([8, 12800])\n",
      "Epoch 15/50, Loss: 1.6430, Train Accuracy: 41.38%, Test Accuracy: 0.00%\n",
      "Shape before flattening: torch.Size([29, 512, 5, 5])\n",
      "Shape after flattening: torch.Size([29, 12800])\n",
      "Shape before flattening: torch.Size([8, 512, 5, 5])\n",
      "Shape after flattening: torch.Size([8, 12800])\n",
      "Epoch 16/50, Loss: 1.6264, Train Accuracy: 48.28%, Test Accuracy: 0.00%\n",
      "Shape before flattening: torch.Size([29, 512, 5, 5])\n",
      "Shape after flattening: torch.Size([29, 12800])\n",
      "Shape before flattening: torch.Size([8, 512, 5, 5])\n",
      "Shape after flattening: torch.Size([8, 12800])\n",
      "Epoch 17/50, Loss: 1.6094, Train Accuracy: 37.93%, Test Accuracy: 0.00%\n",
      "Shape before flattening: torch.Size([29, 512, 5, 5])\n",
      "Shape after flattening: torch.Size([29, 12800])\n",
      "Shape before flattening: torch.Size([8, 512, 5, 5])\n",
      "Shape after flattening: torch.Size([8, 12800])\n",
      "Epoch 18/50, Loss: 1.5910, Train Accuracy: 37.93%, Test Accuracy: 0.00%\n",
      "Shape before flattening: torch.Size([29, 512, 5, 5])\n",
      "Shape after flattening: torch.Size([29, 12800])\n",
      "Shape before flattening: torch.Size([8, 512, 5, 5])\n",
      "Shape after flattening: torch.Size([8, 12800])\n",
      "Epoch 19/50, Loss: 1.5712, Train Accuracy: 37.93%, Test Accuracy: 0.00%\n",
      "Shape before flattening: torch.Size([29, 512, 5, 5])\n",
      "Shape after flattening: torch.Size([29, 12800])\n",
      "Shape before flattening: torch.Size([8, 512, 5, 5])\n",
      "Shape after flattening: torch.Size([8, 12800])\n",
      "Epoch 20/50, Loss: 1.5507, Train Accuracy: 37.93%, Test Accuracy: 0.00%\n",
      "Shape before flattening: torch.Size([29, 512, 5, 5])\n",
      "Shape after flattening: torch.Size([29, 12800])\n",
      "Shape before flattening: torch.Size([8, 512, 5, 5])\n",
      "Shape after flattening: torch.Size([8, 12800])\n",
      "Epoch 21/50, Loss: 1.5300, Train Accuracy: 37.93%, Test Accuracy: 0.00%\n",
      "Shape before flattening: torch.Size([29, 512, 5, 5])\n",
      "Shape after flattening: torch.Size([29, 12800])\n",
      "Shape before flattening: torch.Size([8, 512, 5, 5])\n",
      "Shape after flattening: torch.Size([8, 12800])\n",
      "Epoch 22/50, Loss: 1.5093, Train Accuracy: 37.93%, Test Accuracy: 0.00%\n",
      "Shape before flattening: torch.Size([29, 512, 5, 5])\n",
      "Shape after flattening: torch.Size([29, 12800])\n",
      "Shape before flattening: torch.Size([8, 512, 5, 5])\n",
      "Shape after flattening: torch.Size([8, 12800])\n",
      "Epoch 23/50, Loss: 1.4881, Train Accuracy: 34.48%, Test Accuracy: 0.00%\n",
      "Shape before flattening: torch.Size([29, 512, 5, 5])\n",
      "Shape after flattening: torch.Size([29, 12800])\n",
      "Shape before flattening: torch.Size([8, 512, 5, 5])\n",
      "Shape after flattening: torch.Size([8, 12800])\n",
      "Epoch 24/50, Loss: 1.4653, Train Accuracy: 37.93%, Test Accuracy: 0.00%\n",
      "Shape before flattening: torch.Size([29, 512, 5, 5])\n",
      "Shape after flattening: torch.Size([29, 12800])\n",
      "Shape before flattening: torch.Size([8, 512, 5, 5])\n",
      "Shape after flattening: torch.Size([8, 12800])\n",
      "Epoch 25/50, Loss: 1.4391, Train Accuracy: 44.83%, Test Accuracy: 0.00%\n",
      "Shape before flattening: torch.Size([29, 512, 5, 5])\n",
      "Shape after flattening: torch.Size([29, 12800])\n",
      "Shape before flattening: torch.Size([8, 512, 5, 5])\n",
      "Shape after flattening: torch.Size([8, 12800])\n",
      "Epoch 26/50, Loss: 1.4063, Train Accuracy: 44.83%, Test Accuracy: 0.00%\n",
      "Shape before flattening: torch.Size([29, 512, 5, 5])\n",
      "Shape after flattening: torch.Size([29, 12800])\n",
      "Shape before flattening: torch.Size([8, 512, 5, 5])\n",
      "Shape after flattening: torch.Size([8, 12800])\n",
      "Epoch 27/50, Loss: 1.3787, Train Accuracy: 44.83%, Test Accuracy: 0.00%\n",
      "Shape before flattening: torch.Size([29, 512, 5, 5])\n",
      "Shape after flattening: torch.Size([29, 12800])\n",
      "Shape before flattening: torch.Size([8, 512, 5, 5])\n",
      "Shape after flattening: torch.Size([8, 12800])\n",
      "Epoch 28/50, Loss: 1.3561, Train Accuracy: 44.83%, Test Accuracy: 0.00%\n",
      "Shape before flattening: torch.Size([29, 512, 5, 5])\n",
      "Shape after flattening: torch.Size([29, 12800])\n",
      "Shape before flattening: torch.Size([8, 512, 5, 5])\n",
      "Shape after flattening: torch.Size([8, 12800])\n",
      "Epoch 29/50, Loss: 1.3217, Train Accuracy: 48.28%, Test Accuracy: 0.00%\n",
      "Shape before flattening: torch.Size([29, 512, 5, 5])\n",
      "Shape after flattening: torch.Size([29, 12800])\n",
      "Shape before flattening: torch.Size([8, 512, 5, 5])\n",
      "Shape after flattening: torch.Size([8, 12800])\n",
      "Epoch 30/50, Loss: 1.2994, Train Accuracy: 48.28%, Test Accuracy: 0.00%\n",
      "Shape before flattening: torch.Size([29, 512, 5, 5])\n",
      "Shape after flattening: torch.Size([29, 12800])\n",
      "Shape before flattening: torch.Size([8, 512, 5, 5])\n",
      "Shape after flattening: torch.Size([8, 12800])\n",
      "Epoch 31/50, Loss: 1.2671, Train Accuracy: 48.28%, Test Accuracy: 0.00%\n",
      "Shape before flattening: torch.Size([29, 512, 5, 5])\n",
      "Shape after flattening: torch.Size([29, 12800])\n",
      "Shape before flattening: torch.Size([8, 512, 5, 5])\n",
      "Shape after flattening: torch.Size([8, 12800])\n",
      "Epoch 32/50, Loss: 1.2254, Train Accuracy: 48.28%, Test Accuracy: 0.00%\n",
      "Shape before flattening: torch.Size([29, 512, 5, 5])\n",
      "Shape after flattening: torch.Size([29, 12800])\n",
      "Shape before flattening: torch.Size([8, 512, 5, 5])\n",
      "Shape after flattening: torch.Size([8, 12800])\n",
      "Epoch 33/50, Loss: 1.1955, Train Accuracy: 48.28%, Test Accuracy: 0.00%\n",
      "Shape before flattening: torch.Size([29, 512, 5, 5])\n",
      "Shape after flattening: torch.Size([29, 12800])\n",
      "Shape before flattening: torch.Size([8, 512, 5, 5])\n",
      "Shape after flattening: torch.Size([8, 12800])\n",
      "Epoch 34/50, Loss: 1.1582, Train Accuracy: 51.72%, Test Accuracy: 0.00%\n",
      "Shape before flattening: torch.Size([29, 512, 5, 5])\n",
      "Shape after flattening: torch.Size([29, 12800])\n",
      "Shape before flattening: torch.Size([8, 512, 5, 5])\n",
      "Shape after flattening: torch.Size([8, 12800])\n",
      "Epoch 35/50, Loss: 1.1196, Train Accuracy: 51.72%, Test Accuracy: 0.00%\n",
      "Shape before flattening: torch.Size([29, 512, 5, 5])\n",
      "Shape after flattening: torch.Size([29, 12800])\n",
      "Shape before flattening: torch.Size([8, 512, 5, 5])\n",
      "Shape after flattening: torch.Size([8, 12800])\n",
      "Epoch 36/50, Loss: 1.0880, Train Accuracy: 51.72%, Test Accuracy: 0.00%\n",
      "Shape before flattening: torch.Size([29, 512, 5, 5])\n",
      "Shape after flattening: torch.Size([29, 12800])\n",
      "Shape before flattening: torch.Size([8, 512, 5, 5])\n",
      "Shape after flattening: torch.Size([8, 12800])\n",
      "Epoch 37/50, Loss: 1.0511, Train Accuracy: 51.72%, Test Accuracy: 0.00%\n",
      "Shape before flattening: torch.Size([29, 512, 5, 5])\n",
      "Shape after flattening: torch.Size([29, 12800])\n",
      "Shape before flattening: torch.Size([8, 512, 5, 5])\n",
      "Shape after flattening: torch.Size([8, 12800])\n",
      "Epoch 38/50, Loss: 1.0106, Train Accuracy: 51.72%, Test Accuracy: 0.00%\n",
      "Shape before flattening: torch.Size([29, 512, 5, 5])\n",
      "Shape after flattening: torch.Size([29, 12800])\n",
      "Shape before flattening: torch.Size([8, 512, 5, 5])\n",
      "Shape after flattening: torch.Size([8, 12800])\n",
      "Epoch 39/50, Loss: 0.9718, Train Accuracy: 58.62%, Test Accuracy: 0.00%\n",
      "Shape before flattening: torch.Size([29, 512, 5, 5])\n",
      "Shape after flattening: torch.Size([29, 12800])\n",
      "Shape before flattening: torch.Size([8, 512, 5, 5])\n",
      "Shape after flattening: torch.Size([8, 12800])\n",
      "Epoch 40/50, Loss: 0.9338, Train Accuracy: 62.07%, Test Accuracy: 0.00%\n",
      "Shape before flattening: torch.Size([29, 512, 5, 5])\n",
      "Shape after flattening: torch.Size([29, 12800])\n",
      "Shape before flattening: torch.Size([8, 512, 5, 5])\n",
      "Shape after flattening: torch.Size([8, 12800])\n",
      "Epoch 41/50, Loss: 0.8915, Train Accuracy: 62.07%, Test Accuracy: 0.00%\n",
      "Shape before flattening: torch.Size([29, 512, 5, 5])\n",
      "Shape after flattening: torch.Size([29, 12800])\n",
      "Shape before flattening: torch.Size([8, 512, 5, 5])\n",
      "Shape after flattening: torch.Size([8, 12800])\n",
      "Epoch 42/50, Loss: 0.8471, Train Accuracy: 68.97%, Test Accuracy: 12.50%\n",
      "Shape before flattening: torch.Size([29, 512, 5, 5])\n",
      "Shape after flattening: torch.Size([29, 12800])\n",
      "Shape before flattening: torch.Size([8, 512, 5, 5])\n",
      "Shape after flattening: torch.Size([8, 12800])\n",
      "Epoch 43/50, Loss: 0.8041, Train Accuracy: 82.76%, Test Accuracy: 12.50%\n",
      "Shape before flattening: torch.Size([29, 512, 5, 5])\n",
      "Shape after flattening: torch.Size([29, 12800])\n",
      "Shape before flattening: torch.Size([8, 512, 5, 5])\n",
      "Shape after flattening: torch.Size([8, 12800])\n",
      "Epoch 44/50, Loss: 0.7626, Train Accuracy: 82.76%, Test Accuracy: 12.50%\n",
      "Shape before flattening: torch.Size([29, 512, 5, 5])\n",
      "Shape after flattening: torch.Size([29, 12800])\n",
      "Shape before flattening: torch.Size([8, 512, 5, 5])\n",
      "Shape after flattening: torch.Size([8, 12800])\n",
      "Epoch 45/50, Loss: 0.7212, Train Accuracy: 79.31%, Test Accuracy: 12.50%\n",
      "Shape before flattening: torch.Size([29, 512, 5, 5])\n",
      "Shape after flattening: torch.Size([29, 12800])\n",
      "Shape before flattening: torch.Size([8, 512, 5, 5])\n",
      "Shape after flattening: torch.Size([8, 12800])\n",
      "Epoch 46/50, Loss: 0.6779, Train Accuracy: 79.31%, Test Accuracy: 12.50%\n",
      "Shape before flattening: torch.Size([29, 512, 5, 5])\n",
      "Shape after flattening: torch.Size([29, 12800])\n",
      "Shape before flattening: torch.Size([8, 512, 5, 5])\n",
      "Shape after flattening: torch.Size([8, 12800])\n",
      "Epoch 47/50, Loss: 0.6327, Train Accuracy: 79.31%, Test Accuracy: 12.50%\n",
      "Shape before flattening: torch.Size([29, 512, 5, 5])\n",
      "Shape after flattening: torch.Size([29, 12800])\n",
      "Shape before flattening: torch.Size([8, 512, 5, 5])\n",
      "Shape after flattening: torch.Size([8, 12800])\n",
      "Epoch 48/50, Loss: 0.5891, Train Accuracy: 79.31%, Test Accuracy: 12.50%\n",
      "Shape before flattening: torch.Size([29, 512, 5, 5])\n",
      "Shape after flattening: torch.Size([29, 12800])\n",
      "Shape before flattening: torch.Size([8, 512, 5, 5])\n",
      "Shape after flattening: torch.Size([8, 12800])\n",
      "Epoch 49/50, Loss: 0.5482, Train Accuracy: 86.21%, Test Accuracy: 12.50%\n",
      "Shape before flattening: torch.Size([29, 512, 5, 5])\n",
      "Shape after flattening: torch.Size([29, 12800])\n",
      "Shape before flattening: torch.Size([8, 512, 5, 5])\n",
      "Shape after flattening: torch.Size([8, 12800])\n",
      "Epoch 50/50, Loss: 0.5083, Train Accuracy: 86.21%, Test Accuracy: 12.50%\n"
     ]
    }
   ],
   "source": [
    "# Huấn luyện mô hình\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        logits, _ = model(inputs)  # Chỉ lấy logits\n",
    "        \n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = logits.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_accuracy = 100. * correct / total\n",
    "\n",
    "    # Đánh giá mô hình trên tập kiểm tra\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            logits, _ = model(inputs)  # Chỉ lấy logits\n",
    "            _, predicted = logits.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        test_accuracy = 100. * correct / total\n",
    "    \n",
    "    # Điều chỉnh tốc độ học\n",
    "    scheduler.step(train_loss)\n",
    "\n",
    "    # In thông tin sau mỗi epoch\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, Test Accuracy: {test_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights saved to resnet.pth\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "# Đặt đường dẫn tệp\n",
    "path = 'resnet.pth'\n",
    "\n",
    "# Tạo tệp rỗng nếu chưa tồn tại\n",
    "if not os.path.exists(path):\n",
    "    with open(path, 'w') as file:\n",
    "        pass\n",
    "\n",
    "# Lưu trọng số của mô hình vào tệp\n",
    "torch.save(model.state_dict(), path)\n",
    "print(f\"Model weights saved to {path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class labels: {'Haerin', '2274601080019', 'Danielle', '2274601080005', 'Iroha', '2274601080013', '2274601080006', 'Minji', 'Hyein', 'Hanni'}\n",
      "Number of classes: 10\n",
      "Max label in y_train_encoded: 9\n",
      "Max label in y_test_encoded: 9\n"
     ]
    }
   ],
   "source": [
    "# In ra các giá trị nhãn và số lượng lớp\n",
    "print(f\"Class labels: {set(classNames)}\")\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "\n",
    "# Kiểm tra nhãn đã mã hóa\n",
    "print(f\"Max label in y_train_encoded: {max(y_train_encoded)}\")\n",
    "print(f\"Max label in y_test_encoded: {max(y_test_encoded)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (4): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (5): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AvgPool2d(kernel_size=7, stride=1, padding=2)\n",
       "  (fc): Linear(in_features=12800, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Khởi tạo mô hình với số lớp và độ xám\n",
    "num_classes = len(data_loader.dataset.classes) # Thay đổi tùy theo dữ liệu của bạn\n",
    "grayscale = False  # Thay đổi tùy theo dữ liệu của bạn\n",
    "model = resnet34(num_classes, grayscale)\n",
    "\n",
    "# Tải trọng số từ tệp\n",
    "model_save_path = 'resnet.pth'\n",
    "model.load_state_dict(torch.load(model_save_path))\n",
    "\n",
    "# Đưa mô hình về chế độ đánh giá\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before flattening: torch.Size([1, 512, 5, 5])\n",
      "Shape after flattening: torch.Size([1, 12800])\n"
     ]
    }
   ],
   "source": [
    "# Định nghĩa hàm preprocess_image\n",
    "def preprocess_image(image_path, image_size=(224, 224)):\n",
    "    if not os.path.isfile(image_path):\n",
    "        raise FileNotFoundError(f\"Image file not found: {image_path}\")\n",
    "\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        raise ValueError(f\"Error reading image file: {image_path}\")\n",
    "\n",
    "    img_resized = cv2.resize(img, image_size)\n",
    "    img_rgb = cv2.cvtColor(img_resized, cv2.COLOR_BGR2RGB)\n",
    "    img_tensor = torch.tensor(img_rgb, dtype=torch.float32).permute(2, 0, 1) / 255.0\n",
    "    img_tensor = img_tensor.unsqueeze(0)\n",
    "    \n",
    "    return img_tensor\n",
    "\n",
    "# Định nghĩa hàm predict_image\n",
    "def predict_image(model, img_tensor):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    img_tensor = img_tensor.to(device)\n",
    "    with torch.no_grad():\n",
    "        logits, _ = model(img_tensor)\n",
    "        _, predicted_class = torch.max(logits, 1)\n",
    "    \n",
    "    return predicted_class.item()\n",
    "\n",
    "# Định nghĩa hàm show_prediction\n",
    "def show_prediction(image_path, predicted_class, base_dir):\n",
    "    root = tk.Tk()\n",
    "    root.title(\"Dự Đoán Hình Ảnh\")\n",
    "    root.geometry(\"800x600\")\n",
    "    \n",
    "    img = cv2.imread(image_path)\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img_pil = Image.fromarray(img_rgb)\n",
    "    img_tk = ImageTk.PhotoImage(image=img_pil)\n",
    "    \n",
    "    lbl_img = tk.Label(root, image=img_tk)\n",
    "    lbl_img.pack()\n",
    "    \n",
    "    txt = scrolledtext.ScrolledText(root, width=60, height=10, font=('Arial', 12))\n",
    "    txt.pack()\n",
    "    \n",
    "    # Dự đoán lớp\n",
    "    folder_name = str(predicted_class)\n",
    "    if folder_name in classNames:\n",
    "        index = classNames.index(folder_name)\n",
    "        name = classNames1[index]\n",
    "        info = classInfo.get(name, \"Thông tin không có\")\n",
    "    else:\n",
    "        name = \"Không xác định\"\n",
    "        info = \"Thông tin không có\"\n",
    "    \n",
    "    txt.insert(tk.END, f\"Dự đoán: Lớp {predicted_class}\\nTên: {name}\\nThông tin: {info}\\n\")\n",
    "    \n",
    "    lbl_img.image = img_tk\n",
    "\n",
    "    root.mainloop()\n",
    "\n",
    "# Ví dụ sử dụng\n",
    "image_path = 'Hanni.jpg'\n",
    "try:\n",
    "    img_tensor = preprocess_image(image_path)\n",
    "    predicted_class = predict_image(model, img_tensor)\n",
    "    show_prediction(image_path, predicted_class, path)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "\n",
    "def preprocess_image(image_path, image_size=(224, 224)):\n",
    "    if not os.path.isfile(image_path):\n",
    "        raise FileNotFoundError(f\"Image file not found: {image_path}\")\n",
    "\n",
    "    # Đọc hình ảnh từ file\n",
    "    img = cv2.imread(image_path)\n",
    "    \n",
    "    if img is None:\n",
    "        raise ValueError(f\"Error reading image file: {image_path}\")\n",
    "    \n",
    "    print(f\"Original image shape: {img.shape}\")\n",
    "\n",
    "    # Thay đổi kích thước hình ảnh\n",
    "    img_resized = cv2.resize(img, image_size)\n",
    "    \n",
    "    # Chuyển đổi hình ảnh từ BGR (OpenCV) sang RGB\n",
    "    img_rgb = cv2.cvtColor(img_resized, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Chuyển đổi hình ảnh thành tensor\n",
    "    img_tensor = torch.tensor(img_rgb, dtype=torch.float32).permute(2, 0, 1) / 255.0\n",
    "    img_tensor = img_tensor.unsqueeze(0)  # Thêm batch dimension\n",
    "    \n",
    "    return img_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(model, img_tensor):\n",
    "    # Chuyển mô hình sang thiết bị (CPU/GPU)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    img_tensor = img_tensor.to(device)\n",
    "    with torch.no_grad():\n",
    "        logits, probas = model(img_tensor)\n",
    "        _, predicted_class = torch.max(logits, 1)\n",
    "    \n",
    "    return predicted_class.item(), probas[0, predicted_class].item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import scrolledtext\n",
    "from PIL import Image, ImageTk\n",
    "\n",
    "def show_prediction(image_path, predicted_class, base_dir):\n",
    "    root = tk.Tk()\n",
    "    root.title(\"Dự Đoán Hình Ảnh\")\n",
    "    root.geometry(\"800x600\")\n",
    "    \n",
    "    # Đọc và hiển thị hình ảnh\n",
    "    img = cv2.imread(image_path)\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img_pil = Image.fromarray(img_rgb)\n",
    "    img_tk = ImageTk.PhotoImage(image=img_pil)\n",
    "    \n",
    "    lbl_img = tk.Label(root, image=img_tk)\n",
    "    lbl_img.pack()\n",
    "    \n",
    "    # Hiển thị kết quả dự đoán\n",
    "    txt = scrolledtext.ScrolledText(root, width=60, height=10, font=('Arial', 12))\n",
    "    txt.pack()\n",
    "    \n",
    "    # Tìm thư mục tương ứng với lớp dự đoán\n",
    "    folder_path = get_folder_path_from_class_name(predicted_class, base_dir)\n",
    "    if folder_path:\n",
    "        class_info = load_class_info_from_folder(folder_path)\n",
    "    else:\n",
    "        class_info = \"Thông tin không có\"\n",
    "    \n",
    "    txt.insert(tk.END, f\"Dự đoán: Lớp {predicted_class}\\nThông tin: {class_info}\\n\")\n",
    "    \n",
    "    # Lưu tham chiếu đến đối tượng ImageTk\n",
    "    lbl_img.image = img_tk\n",
    "\n",
    "    root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_folder_path_from_class_name(class_index, base_dir):\n",
    "    folder_name = str(class_index)\n",
    "    folder_path = os.path.join(base_dir, folder_name)\n",
    "    if os.path.isdir(folder_path):\n",
    "        return folder_path\n",
    "    return None\n",
    "\n",
    "def load_class_info_from_folder(folder_path):\n",
    "    info_file_path = os.path.join(folder_path, 'info.txt')\n",
    "    if os.path.isfile(info_file_path):\n",
    "        with open(info_file_path, 'r') as file:\n",
    "            return file.read()\n",
    "    return \"Thông tin không có\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original image shape: (780, 624, 3)\n",
      "Shape before flattening: torch.Size([1, 512, 5, 5])\n",
      "Shape after flattening: torch.Size([1, 12800])\n"
     ]
    }
   ],
   "source": [
    "image_path = 'Hanni.jpg'\n",
    "base_dir = 'student_images'\n",
    "\n",
    "try:\n",
    "    img_tensor = preprocess_image(image_path)\n",
    "    predicted_class = predict_image(model, img_tensor)\n",
    "    show_prediction(image_path, predicted_class, base_dir)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nhận dạng bằng ảnh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Đọc hình ảnh từ file để nhận diện\n",
    "test_image_path = 'Hannie.jpg'\n",
    "img = cv2.imread(test_image_path)\n",
    "\n",
    "# Thay đổi kích thước hình ảnh xuống còn 1/4 kích thước để xử lý nhanh hơn\n",
    "imgS = cv2.resize(img, (0, 0), None, 0.25, 0.25)\n",
    "\n",
    "# Chuyển đổi hình ảnh từ BGR (định dạng OpenCV) sang RGB (định dạng face_recognition)\n",
    "imgS = cv2.cvtColor(imgS, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Tìm tất cả các vị trí khuôn mặt trong hình ảnh\n",
    "faces_in_frame = face_recognition.face_locations(imgS)\n",
    "\n",
    "# Mã hóa các khuôn mặt được tìm thấy trong hình ảnh\n",
    "encoded_faces = face_recognition.face_encodings(imgS, faces_in_frame)\n",
    "\n",
    "# Tạo cửa sổ tkinter\n",
    "root = tk.Tk()\n",
    "root.title(\"Thông tin Sinh Viên\")\n",
    "root.geometry(\"1920x1080\")\n",
    "\n",
    "# Tạo vùng văn bản có thể cuộn\n",
    "txt = scrolledtext.ScrolledText(root, width=40, height=20, font=('Arial', 12))  # Sử dụng font hỗ trợ Unicode\n",
    "txt.pack(side=tk.RIGHT, fill=tk.BOTH, expand=True)\n",
    "\n",
    "# Tạo một nhãn để hiển thị hình ảnh\n",
    "lbl = tk.Label(root)\n",
    "lbl.pack(side=tk.LEFT)\n",
    "\n",
    "# Biến lưu trữ số lượng khuôn mặt nhận diện\n",
    "num_detected_faces = 0\n",
    "\n",
    "# Lặp qua từng khuôn mặt được mã hóa và vị trí của nó\n",
    "for encode_face, faceloc in zip(encoded_faces, faces_in_frame):\n",
    "    # So sánh khuôn mặt được mã hóa với các khuôn mặt đã biết\n",
    "    matches = face_recognition.compare_faces(encoded_face_train, encode_face)\n",
    "    \n",
    "    # Tính khoảng cách giữa khuôn mặt được mã hóa và các khuôn mặt đã biết\n",
    "    faceDist = face_recognition.face_distance(encoded_face_train, encode_face)\n",
    "    \n",
    "    # Tìm chỉ số của khoảng cách nhỏ nhất (khuôn mặt giống nhất)\n",
    "    matchIndex = np.argmin(faceDist)\n",
    "    \n",
    "    # Nếu tìm thấy sự phù hợp, tiến hành chú thích\n",
    "    if matches[matchIndex]:\n",
    "        name = classNames[matchIndex].upper()\n",
    "        name1 = classNames1[matchIndex].upper()\n",
    "        \n",
    "        # Trích xuất vị trí khuôn mặt và mở rộng lại kích thước ban đầu\n",
    "        y1, x2, y2, x1 = faceloc\n",
    "        y1, x2, y2, x1 = y1 * 4, x2 * 4, y2 * 4, x1 * 4\n",
    "        \n",
    "        # Vẽ một hình chữ nhật xung quanh khuôn mặt\n",
    "        cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        \n",
    "        # Vẽ một hình chữ nhật đầy màu dưới khuôn mặt để hiển thị tên\n",
    "        cv2.rectangle(img, (x1, y2 - 35), (x2, y2), (0, 255, 0), cv2.FILLED)\n",
    "        \n",
    "        # Đặt tên của người đó dưới khuôn mặt\n",
    "        cv2.putText(img, name, (x1 + 6, y2 - 5), cv2.FONT_HERSHEY_COMPLEX, 1, (255, 255, 255), 2)\n",
    "        \n",
    "        # Lấy thời gian hiện tại\n",
    "        current_time = datetime.now().strftime(f\"- Ngày %d-%m-%Y \\n - Giờ %H:%M:%S\")\n",
    "        # Hiển thị thông tin từ tệp .txt trong giao diện tkinter\n",
    "\n",
    "        info = classInfo.get(classNames1[matchIndex])\n",
    "        # Hiển thị thông tin từ tệp .txt và thời gian nhận diện trong giao diện tkinter\n",
    "        txt.insert(tk.END, f\" Tên: {name1} \\n MSSV: {info} \\n Nhận dạng lúc: \\n {current_time}\\n\\n\")\n",
    "        \n",
    "        # Tăng biến đếm số lượng khuôn mặt nhận diện\n",
    "        num_detected_faces += 1\n",
    "\n",
    "# Kiểm tra nếu không có khuôn mặt nào được nhận diện\n",
    "if num_detected_faces == 0:\n",
    "    txt.insert(tk.END, \"Không nhận diện được khuôn mặt nào trong ảnh.\\n\")\n",
    "\n",
    "# Chuyển đổi hình ảnh từ BGR sang RGB để sử dụng với PIL\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "img_pil = Image.fromarray(img)\n",
    "img_tk = ImageTk.PhotoImage(image=img_pil)\n",
    "\n",
    "# Đặt hình ảnh vào nhãn\n",
    "lbl.config(image=img_tk)\n",
    "lbl.image = img_tk\n",
    "\n",
    "# Khởi động giao diện tkinter\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nhận dạng bằng camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tạo cửa sổ tkinter\n",
    "root = tk.Tk()\n",
    "root.title(\"Thông tin Sinh Viên\")\n",
    "root.geometry(\"1920x1080\")\n",
    "\n",
    "# Tạo vùng văn bản có thể cuộn\n",
    "txt = scrolledtext.ScrolledText(root, width=40, height=20, font=('Arial', 12))  # Sử dụng font hỗ trợ Unicode\n",
    "txt.pack(side=tk.RIGHT, fill=tk.BOTH, expand=True)\n",
    "\n",
    "# Tạo một nhãn để hiển thị hình ảnh\n",
    "lbl = tk.Label(root)\n",
    "lbl.pack(side=tk.LEFT)\n",
    "\n",
    "# Biến điều khiển vòng lặp video\n",
    "stop_video = threading.Event()\n",
    "\n",
    "# Hàm xử lý từng khung hình từ camera\n",
    "def process_frame(frame):\n",
    "    # Thay đổi kích thước hình ảnh xuống còn 1/4 kích thước để xử lý nhanh hơn\n",
    "    imgS = cv2.resize(frame, (0, 0), None, 0.25, 0.25)\n",
    "\n",
    "    # Chuyển đổi hình ảnh từ BGR (định dạng OpenCV) sang RGB (định dạng face_recognition)\n",
    "    imgS = cv2.cvtColor(imgS, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Tìm tất cả các vị trí khuôn mặt trong hình ảnh\n",
    "    faces_in_frame = face_recognition.face_locations(imgS)\n",
    "\n",
    "    # Mã hóa các khuôn mặt được tìm thấy trong hình ảnh\n",
    "    encoded_faces = face_recognition.face_encodings(imgS, faces_in_frame)\n",
    "\n",
    "    # Biến lưu trữ số lượng khuôn mặt nhận diện\n",
    "    num_detected_faces = 0\n",
    "\n",
    "    # Lặp qua từng khuôn mặt được mã hóa và vị trí của nó\n",
    "    for encode_face, faceloc in zip(encoded_faces, faces_in_frame):\n",
    "        # So sánh khuôn mặt được mã hóa với các khuôn mặt đã biết\n",
    "        matches = face_recognition.compare_faces(encoded_face_train, encode_face)\n",
    "\n",
    "        # Tính khoảng cách giữa khuôn mặt được mã hóa và các khuôn mặt đã biết\n",
    "        faceDist = face_recognition.face_distance(encoded_face_train, encode_face)\n",
    "\n",
    "        # Tìm chỉ số của khoảng cách nhỏ nhất (khuôn mặt giống nhất)\n",
    "        matchIndex = np.argmin(faceDist)\n",
    "\n",
    "        # Nếu tìm thấy sự phù hợp, tiến hành chú thích\n",
    "        if matches[matchIndex]:\n",
    "            name = classNames[matchIndex].upper()\n",
    "            name1 = classNames1[matchIndex].upper()\n",
    "\n",
    "            # Trích xuất vị trí khuôn mặt và mở rộng lại kích thước ban đầu\n",
    "            y1, x2, y2, x1 = faceloc\n",
    "            y1, x2, y2, x1 = y1 * 4, x2 * 4, y2 * 4, x1 * 4\n",
    "\n",
    "            # Vẽ một hình chữ nhật xung quanh khuôn mặt\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "            # Vẽ một hình chữ nhật đầy màu dưới khuôn mặt để hiển thị tên\n",
    "            cv2.rectangle(frame, (x1, y2 - 35), (x2, y2), (0, 255, 0), cv2.FILLED)\n",
    "\n",
    "            # Đặt tên của người đó dưới khuôn mặt\n",
    "            cv2.putText(frame, name, (x1 + 6, y2 - 5), cv2.FONT_HERSHEY_COMPLEX, 1, (255, 255, 255), 2)\n",
    "\n",
    "            # Lấy thời gian hiện tại\n",
    "            current_time = datetime.now().strftime(f\"- Ngày %d-%m-%Y \\n - Giờ %H:%M:%S\")\n",
    "\n",
    "            # Hiển thị thông tin từ tệp .txt và thời gian nhận diện trong giao diện tkinter\n",
    "            info = classInfo.get(classNames1[matchIndex])\n",
    "            txt.insert(tk.END, f\" Tên: {name1} \\n MSSV: {info} \\n Nhận dạng lúc: \\n {current_time}\\n\\n\")\n",
    "\n",
    "    # Chuyển đổi hình ảnh từ BGR sang RGB để sử dụng với PIL\n",
    "    img_pil = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    img_tk = ImageTk.PhotoImage(image=img_pil)\n",
    "\n",
    "    # Đặt hình ảnh vào nhãn\n",
    "    lbl.config(image=img_tk)\n",
    "    lbl.image = img_tk\n",
    "\n",
    "# Hàm lấy khung hình từ camera và gọi hàm xử lý khung hình\n",
    "def video_loop():\n",
    "    while not stop_video.is_set():\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            process_frame(frame)\n",
    "        root.update_idletasks()\n",
    "        root.update()\n",
    "\n",
    "# Hàm lắng nghe phím nhấn 'q' để thoát\n",
    "def listen_for_key():\n",
    "    keyboard.add_hotkey('q', stop_video.set)\n",
    "\n",
    "# Mở camera\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Tạo và bắt đầu các luồng\n",
    "video_thread = threading.Thread(target=video_loop)\n",
    "key_listener_thread = threading.Thread(target=listen_for_key)\n",
    "\n",
    "video_thread.start()\n",
    "key_listener_thread.start()\n",
    "\n",
    "# Khởi động giao diện tkinter\n",
    "root.mainloop()\n",
    "\n",
    "# Đợi cho các luồng kết thúc\n",
    "video_thread.join()\n",
    "key_listener_thread.join()\n",
    "\n",
    "# Giải phóng camera khi kết thúc\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Camera cũ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Khởi tạo các biến\n",
    "images = []\n",
    "classNames = []\n",
    "classInfo = {}  # Từ điển để lưu thông tin từ tệp .txt\n",
    "classNames1 = []\n",
    "\n",
    "# Đường dẫn đến thư mục chứa hình ảnh và tệp tin\n",
    "\n",
    "# Duyệt qua các thư mục và tệp tin trong `path`\n",
    "for root, dirs, files in os.walk(path):\n",
    "    for cl in dirs:  # Duyệt qua các thư mục con\n",
    "        class_path = os.path.join(root, cl)\n",
    "        for img_name in os.listdir(class_path):\n",
    "            img_path = os.path.join(class_path, img_name)\n",
    "            if img_name.endswith(('png', 'jpg', 'jpeg')):\n",
    "                curImg = cv2.imread(img_path)\n",
    "                if curImg is not None:\n",
    "                    images.append(curImg)\n",
    "                    txt_path = os.path.join(class_path, cl + '.txt')\n",
    "                    if os.path.exists(txt_path):\n",
    "                        with open(txt_path, 'r', encoding='utf-8') as file:\n",
    "                            content = file.read().strip().splitlines()\n",
    "                            name = content[0].strip()  # Lấy tên từ dòng đầu tiên\n",
    "                            info = '\\n'.join(content[1:]).strip()  # Lấy thông tin từ các dòng còn lại\n",
    "                            classNames.append(cl)\n",
    "                            classNames1.append(name)\n",
    "                            classInfo[name] = info\n",
    "                    else:\n",
    "                        print(f\"Error: Could not find the text file {txt_path}\")\n",
    "                else:\n",
    "                    print(f\"Error: Could not load image {img_path}\")\n",
    "\n",
    "# Hiển thị số lượng hình ảnh đã được nạp\n",
    "print(f\"Loaded {len(images)} images and corresponding names and info.\")\n",
    "\n",
    "# Mã hóa tất cả các khuôn mặt trong images và lưu vào danh sách\n",
    "encoded_face_train = [face_recognition.face_encodings(img)[0] for img in images if len(face_recognition.face_encodings(img)) > 0]\n",
    "\n",
    "# Kiểm tra nếu không có khuôn mặt nào được mã hóa\n",
    "if not encoded_face_train:\n",
    "    print(\"No faces were encoded. Exiting program.\")\n",
    "    exit()\n",
    "\n",
    "# Tạo cửa sổ tkinter\n",
    "root = tk.Tk()\n",
    "root.title(\"Thông tin Sinh Viên\")\n",
    "root.geometry(\"1920x1080\")\n",
    "\n",
    "# Tạo vùng văn bản có thể cuộn\n",
    "txt = scrolledtext.ScrolledText(root, width=40, height=20, font=('Arial', 12))  # Sử dụng font hỗ trợ Unicode\n",
    "txt.pack(side=tk.RIGHT, fill=tk.BOTH, expand=True)\n",
    "\n",
    "# Tạo một nhãn để hiển thị hình ảnh\n",
    "lbl = tk.Label(root)\n",
    "lbl.pack(side=tk.LEFT)\n",
    "\n",
    "# Biến điều khiển vòng lặp video\n",
    "stop_video = threading.Event()\n",
    "\n",
    "# Hàm xử lý từng khung hình từ camera\n",
    "def process_frame(frame):\n",
    "    # Thay đổi kích thước hình ảnh xuống còn 1/4 kích thước để xử lý nhanh hơn\n",
    "    imgS = cv2.resize(frame, (0, 0), None, 0.25, 0.25)\n",
    "\n",
    "    # Chuyển đổi hình ảnh từ BGR (định dạng OpenCV) sang RGB (định dạng face_recognition)\n",
    "    imgS = cv2.cvtColor(imgS, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Tìm tất cả các vị trí khuôn mặt trong hình ảnh\n",
    "    faces_in_frame = face_recognition.face_locations(imgS)\n",
    "\n",
    "    # Mã hóa các khuôn mặt được tìm thấy trong hình ảnh\n",
    "    encoded_faces = face_recognition.face_encodings(imgS, faces_in_frame)\n",
    "\n",
    "    # Biến lưu trữ số lượng khuôn mặt nhận diện\n",
    "    num_detected_faces = 0\n",
    "\n",
    "    # Lặp qua từng khuôn mặt được mã hóa và vị trí của nó\n",
    "    for encode_face, faceloc in zip(encoded_faces, faces_in_frame):\n",
    "        # So sánh khuôn mặt được mã hóa với các khuôn mặt đã biết\n",
    "        matches = face_recognition.compare_faces(encoded_face_train, encode_face)\n",
    "\n",
    "        # Tính khoảng cách giữa khuôn mặt được mã hóa và các khuôn mặt đã biết\n",
    "        faceDist = face_recognition.face_distance(encoded_face_train, encode_face)\n",
    "\n",
    "        # Tìm chỉ số của khoảng cách nhỏ nhất (khuôn mặt giống nhất)\n",
    "        matchIndex = np.argmin(faceDist)\n",
    "\n",
    "        # Nếu tìm thấy sự phù hợp, tiến hành chú thích\n",
    "        if matches[matchIndex]:\n",
    "            name = classNames[matchIndex].upper()\n",
    "            name1 = classNames1[matchIndex].upper()\n",
    "\n",
    "            # Trích xuất vị trí khuôn mặt và mở rộng lại kích thước ban đầu\n",
    "            y1, x2, y2, x1 = faceloc\n",
    "            y1, x2, y2, x1 = y1 * 4, x2 * 4, y2 * 4, x1 * 4\n",
    "\n",
    "            # Vẽ một hình chữ nhật xung quanh khuôn mặt\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "            # Vẽ một hình chữ nhật đầy màu dưới khuôn mặt để hiển thị tên\n",
    "            cv2.rectangle(frame, (x1, y2 - 35), (x2, y2), (0, 255, 0), cv2.FILLED)\n",
    "\n",
    "            # Đặt tên của người đó dưới khuôn mặt\n",
    "            cv2.putText(frame, name, (x1 + 6, y2 - 5), cv2.FONT_HERSHEY_COMPLEX, 1, (255, 255, 255), 2)\n",
    "\n",
    "            # Lấy thời gian hiện tại\n",
    "            current_time = datetime.now().strftime(f\"- Ngày %d-%m-%Y \\n - Giờ %H:%M:%S\")\n",
    "            \n",
    "            # Hiển thị thông tin từ tệp .txt và thời gian nhận diện trong giao diện tkinter\n",
    "            info = classInfo.get(classNames1[matchIndex])\n",
    "            txt.insert(tk.END, f\" Tên: {name1} \\n MSSV: {info} \\n Nhận dạng lúc: \\n {current_time}\\n\\n\")\n",
    "\n",
    "    # Chuyển đổi hình ảnh từ BGR sang RGB để sử dụng với PIL\n",
    "    img_pil = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    img_tk = ImageTk.PhotoImage(image=img_pil)\n",
    "\n",
    "    # Đặt hình ảnh vào nhãn\n",
    "    lbl.config(image=img_tk)\n",
    "    lbl.image = img_tk\n",
    "\n",
    "# Hàm lấy khung hình từ camera và gọi hàm xử lý khung hình\n",
    "def video_loop():\n",
    "    while not stop_video.is_set():\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            process_frame(frame)\n",
    "        root.update_idletasks()\n",
    "        root.update()\n",
    "\n",
    "# Hàm lắng nghe phím nhấn 'q' để thoát\n",
    "def listen_for_key():\n",
    "    keyboard.add_hotkey('q', stop_video.set)\n",
    "\n",
    "# Mở camera\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Tạo và bắt đầu các luồng\n",
    "video_thread = threading.Thread(target=video_loop)\n",
    "key_listener_thread = threading.Thread(target=listen_for_key)\n",
    "\n",
    "video_thread.start()\n",
    "key_listener_thread.start()\n",
    "\n",
    "# Khởi động giao diện tkinter\n",
    "root.mainloop()\n",
    "\n",
    "# Đợi cho các luồng kết thúc\n",
    "video_thread.join()\n",
    "key_listener_thread.join()\n",
    "\n",
    "# Giải phóng camera khi kết thúc\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.models import resnet34, ResNet34_Weights\n",
    "\n",
    "# Khởi tạo mô hình ResNet-34 với weights\n",
    "weights = ResNet34_Weights.DEFAULT\n",
    "model = resnet34(weights=weights)\n",
    "\n",
    "# Số lớp đầu ra (tùy thuộc vào dữ liệu của bạn)\n",
    "num_classes = len(datasets.ImageFolder('student_images/train').classes)\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "\n",
    "# Chuyển mô hình sang thiết bị (CPU/GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Định nghĩa criterion và optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Kỹ thuật giảm tốc độ học\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5)\n",
    "\n",
    "# Callback: Lưu trọng số mô hình tốt nhất\n",
    "best_acc = 0.0\n",
    "model_save_path = 'best_model.pth'\n",
    "\n",
    "# Callback: Early Stopping\n",
    "patience = 5\n",
    "early_stop_counter = 0\n",
    "\n",
    "# Định nghĩa biến đổi dữ liệu\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Khởi tạo DataLoader\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.ImageFolder('student_images/train', transform=transform),\n",
    "    batch_size=32,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.ImageFolder('student_images/test', transform=transform),\n",
    "    batch_size=32,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Huấn luyện mô hình\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_accuracy = 100. * correct / total\n",
    "\n",
    "    # Đánh giá mô hình trên tập kiểm tra\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        test_accuracy = 100. * correct / total\n",
    "    \n",
    "    # Điều chỉnh tốc độ học\n",
    "    scheduler.step(train_loss)\n",
    "    \n",
    "    # Callback: Lưu trọng số mô hình tốt nhất\n",
    "    if test_accuracy > best_acc:\n",
    "        best_acc = test_accuracy\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "        print(f\"Model weights saved to {model_save_path}\")\n",
    "        early_stop_counter = 0  # Reset counter if model improves\n",
    "    else:\n",
    "        early_stop_counter += 1\n",
    "    \n",
    "    # Callback: Early Stopping\n",
    "    if early_stop_counter >= patience:\n",
    "        print(f\"Early stopping at epoch {epoch+1}\")\n",
    "        break\n",
    "\n",
    "    # In thông tin sau mỗi epoch\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, Test Accuracy: {test_accuracy:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
